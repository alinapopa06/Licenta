AdaBoost refers to a particular method of training a boosted classifier. A boost classifier is a classifier where each ft is a weak learner that takes an object x as input and returns a value indicating the class of the object. For example, in the two-class problem, the sign of the weak learner output identifies the predicted object class and the absolute value gives the confidence in that classification. Similarly, the Tth classifier is positive if the sample is in a positive class and negative otherwise. Each weak learner produces an output hypothesis, h(xi), for each sample in the training set. At each iteration t, a weak learner is selected and assigned a coefficient αt such that the sum training error Et of the resulting t-stage boost classifier is minimized. Et = ∑E[Ft−1(xi)+αt*h(xi)]. Here Ft−1(x) is the boosted classifier that has been built up to the previous stage of training, E(F) is some error function and ft(x)=αt*h(x) is the weak learner that is being considered for addition to the final classifier.